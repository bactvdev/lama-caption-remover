from pathlib import Path
import os
from natsort import natsorted
from PIL import Image
import numpy as np
import cv2
import torch
import easyocr
from omegaconf import OmegaConf
from saicinpainting.training.trainers import load_checkpoint
from saicinpainting.evaluation.data import pad_img_to_modulo
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import subprocess

# --- Load model LaMa ---
config_path = 'models/big-lama/config.yaml'
checkpoint_path = 'models/big-lama/models/best.ckpt'

config = OmegaConf.load(config_path)
config.training_model.predict_only = True
config.visualizer.kind = 'noop'

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = load_checkpoint(config, checkpoint_path, strict=False, map_location=device)
model.to(device).eval()

# --- EasyOCR ---
# reader = easyocr.Reader(['ch_sim', 'en'], gpu=torch.cuda.is_available())

def is_dir_not_empty(path):
    return os.path.isdir(path) and any(os.scandir(path))

def split_video_to_frames(input_file) -> None:
    print('process split video to frames')
    output_dir =  Path('frames')
    output_dir.mkdir(parents=True, exist_ok=True)

    # cap = cv2.VideoCapture(str(input_file))
    # frame_num = 0

    # while True:
    #     ret, frame = cap.read()
    #     if not ret:
    #         break
    #     cv2.imwrite(f"{output_dir}/frame_{frame_num:05d}.png", frame)
    #     frame_num += 1

    # cap.release()
    # TÃ¡ch frame ra thÃ nh JPEG cháº¥t lÆ°á»£ng tá»‘t
    command = [
        'ffmpeg',
        '-i', str(input_file),
        '-qscale:v', '2',
        f'{output_dir}/frame_%05d.jpg'
    ]
    subprocess.run(command, check=True)

# def remove_video_caption(input_file): 
#     cap = cv2.VideoCapture(str(input_file))

#     # Láº¥y FPS (frames per second)
#     fps = cap.get(cv2.CAP_PROP_FPS)

#     frames_dir = Path('frames')
#     if not is_dir_not_empty(frames_dir):
#         split_video_to_frames(input_file)

#     frames_dir_output = Path('frames_editted')
#     frames_dir_output.mkdir(parents=True, exist_ok=True)

#     # task_list = []
#     for file_name in os.listdir(frames_dir):
#         if file_name.endswith('.png') or file_name.endswith('.jpg'):
#             image = os.path.join(frames_dir, file_name)
#             if not os.path.isfile(image):
#                 continue
            
#             image_output_path = os.path.join(frames_dir_output, file_name)

#             if not os.path.exists(image_output_path):
#                 _process_remove_caption_on_frame(image, image_output_path)

#     return frames_to_video(
#         frame_dir=frames_dir_output,
#         fps=fps,
#     )

def remove_video_caption(input_file): 
    cap = cv2.VideoCapture(str(input_file))
    fps = cap.get(cv2.CAP_PROP_FPS)

    frames_dir = Path('frames')
    if not is_dir_not_empty(frames_dir):
        split_video_to_frames(input_file)

    frames_dir_output = Path('frames_editted')
    frames_dir_output.mkdir(parents=True, exist_ok=True)

    # Táº¡o danh sÃ¡ch cÃ¡c cáº·p (input_image, output_image)
    tasks = []
    for file_name in os.listdir(frames_dir):
        if file_name.lower().endswith(('.png', '.jpg')):
            input_path = frames_dir / file_name
            output_path = frames_dir_output / file_name
            if not output_path.exists():
                tasks.append((str(input_path), str(output_path)))

    # Cháº¡y xá»­ lÃ½ song song
    max_workers = min(8, os.cpu_count()) # type: ignore
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        executor.map(_process_remove_caption_on_frame, tasks)

    return frames_to_video(frame_dir=frames_dir_output, fps=fps)

def _process_remove_caption_on_frame(image_path_output_tuple):
    reader = easyocr.Reader(['ch_sim', 'en'], gpu=torch.cuda.is_available())

    input_image, output_image = image_path_output_tuple
    print('process image:', str(input_image))

    image_pil = Image.open(input_image).convert("RGB")

    # Táº¡o mask báº±ng hÃ m báº¡n viáº¿t
    mask_pil = generate_mask_from_text(reader, image_pil)

    # Inpaint báº±ng OpenCV
    image_np = np.array(image_pil)
    mask_np = np.array(mask_pil)
    inpainted_np = cv2.inpaint(image_np, mask_np, inpaintRadius=5, flags=cv2.INPAINT_NS)

    # Chuyá»ƒn sang PIL Ä‘á»ƒ dÃ¹ng vá»›i LaMa
    inpainted_pil = Image.fromarray(inpainted_np)

    final_image = run_lama(reader, inpainted_pil, mask_pil)
    final_image.save(output_image, format="PNG")

    # image_np = cv2.imread(input_image)
    # if image_np is None:
    #     print(f"âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c áº£nh: {input_image}")
    #     return

    # results = reader.readtext(image_np)
    # mask_dilated = np.zeros(image_np.shape[:2], dtype=np.uint8)

    # for (bbox, text, prob) in results:
    #     pts = np.array(bbox, dtype=np.int32)
    #     cv2.fillPoly(mask_dilated, [pts], 255)

    # inpainted_np = cv2.inpaint(image_np, mask_dilated, inpaintRadius=5, flags=cv2.INPAINT_NS)
    # inpainted_pil = Image.fromarray(inpainted_np)
    # mask_pil = Image.fromarray(mask_dilated)
    # final_image = run_lama(reader, inpainted_pil, mask_pil)
    # final_image.save(output_image, format="PNG")

# def _process_remove_caption_on_frame(input_image, output_image):
#     print('process image: ', str(input_image))
#     image_np = cv2.imread(input_image)
#     # image_np = np.array(input_image)

#     # DÃ¹ng OCR Ä‘á»ƒ phÃ¡t hiá»‡n caption (text)
#     results = reader.readtext(image_np)

#     # Táº¡o mask rá»—ng (Ä‘en hoÃ n toÃ n)
#     mask_dilated = np.zeros(image_np.shape[:2], dtype=np.uint8)

#     for (bbox, text, prob) in results:
#         # bbox: [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]
#         pts = np.array(bbox, dtype=np.int32)
#         # TÃ´ vÃ¹ng chá»©a chá»¯ lÃªn mask (tráº¯ng: 255)
#         cv2.fillPoly(mask_dilated, [pts], 255) # type: ignore

#     # XoÃ¡ caption báº±ng inpainting
#     inpainted_np = cv2.inpaint(image_np, mask_dilated, inpaintRadius=5, flags=cv2.INPAINT_NS)

#     # 5. DÃ¹ng LaMa refine láº¡i vÃ¹ng Ä‘Ã³ (PIL + mask)
#     inpainted_pil = Image.fromarray(inpainted_np)
#     mask_pil = Image.fromarray(mask_dilated)

#     final_image = run_lama(inpainted_pil, mask_pil)

#     # Tráº£ vá» file áº£nh
#     final_image.save(output_image, format="PNG")

# def frames_to_video(frame_dir, fps=30.0):
#     print('frames to video')

#     # Láº¥y danh sÃ¡ch file áº£nh (chá»‰ láº¥y .png, .jpg)
#     frame_files = [f for f in os.listdir(frame_dir) if f.lower().endswith(('.png', '.jpg'))]
#     frame_files = natsorted(frame_files)  # Äáº£m báº£o Ä‘Ãºng thá»© tá»± khung hÃ¬nh

#     if not frame_files:
#         print("âŒ KhÃ´ng tÃ¬m tháº¥y áº£nh trong thÆ° má»¥c.")
#         return

#     output_path = Path('video-editted.mp4')

#     # Äá»c kÃ­ch thÆ°á»›c áº£nh Ä‘áº§u tiÃªn
#     first_frame_path = os.path.join(frame_dir, frame_files[0])
#     frame = cv2.imread(first_frame_path)
#     height, width, _ = frame.shape

#     # Táº¡o Ä‘á»‘i tÆ°á»£ng VideoWriter
#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # type: ignore # hoáº·c 'XVID'
#     out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

#     print(f"â–¶ï¸ Báº¯t Ä‘áº§u ghÃ©p {len(frame_files)} áº£nh thÃ nh video...")

#     for file in frame_files:
#         frame_path = os.path.join(frame_dir, file)
#         img = cv2.imread(frame_path)
#         if img is None:
#             print(f"âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c áº£nh: {file}")
#             continue
#         out.write(img)

#     out.release()
#     return output_path

def frames_to_video_ffmpeg(frame_dir, fps=30):
    frame_dir = Path(frame_dir)
    output_path = Path('video_editted.mp4')

    # Kiá»ƒm tra cÃ³ frame khÃ´ng
    frame_files = [f for f in os.listdir(frame_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    if not frame_files:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y áº£nh trong thÆ° má»¥c.")
        return None

    # Äáº£m báº£o thá»© tá»± vÃ  tÃªn áº£nh Ä‘Ãºng Ä‘á»‹nh dáº¡ng ffmpeg (frame_00001.jpg)
    frame_files = natsorted(frame_files)
    first_file = frame_files[0]
    if not first_file.startswith('frame_') or not '%05d' in first_file:
        print("âš ï¸ FFmpeg yÃªu cáº§u tÃªn file theo máº«u: frame_%05d.jpg (frame_00001.jpg, ...)")
        print("ðŸ’¡ Báº¡n nÃªn Ä‘á»•i tÃªn trÆ°á»›c khi dÃ¹ng FFmpeg.")
        return None

    # Cháº¡y FFmpeg Ä‘á»ƒ ghÃ©p video
    input_pattern = str(frame_dir / 'frame_%05d.jpg')  # hoáº·c .png náº¿u báº¡n dÃ¹ng PNG
    command = [
        'ffmpeg',
        '-y',                        # Ghi Ä‘Ã¨ náº¿u file Ä‘Ã£ tá»“n táº¡i
        '-framerate', str(fps),
        '-i', input_pattern,
        '-c:v', 'libx264',
        '-pix_fmt', 'yuv420p',
        str(output_path)
    ]

    print(f"â–¶ï¸ Äang ghÃ©p video tá»« {len(frame_files)} áº£nh...")
    subprocess.run(command, check=True)
    print(f"âœ… Video Ä‘Ã£ lÆ°u táº¡i: {output_path}")
    return output_path

# --- Táº¡o mask tá»± Ä‘á»™ng tá»« caption ---
def generate_mask_from_text(reader, image: Image.Image) -> Image.Image:
    image_np = np.array(image.convert("RGB"))
    h, w = image_np.shape[:2]
    results = reader.readtext(image_np)
    mask = np.zeros((h, w), dtype=np.uint8)

    for (bbox, text, prob) in results:
        print(f"Detected: {text}, confidence: {prob:.2f}")  # ðŸ‘ˆ debug
        if prob < 0.5:
            continue
        pts = np.array(bbox, dtype=np.int32)
        cv2.fillPoly(mask, [pts], 255)

    mask_image = Image.fromarray(mask)
    mask_image.save("mask_debug.png")  # ðŸ‘ˆ xem thá»­ mask
    return mask_image

# --- Xá»­ lÃ½ inpaint báº±ng LaMa ---
def run_lama(reader, image: Image.Image, mask: Image.Image) -> Image.Image:
    # Convert PIL â†’ numpy
    img = np.array(image.convert("RGB"))  # shape: (H, W, 3)
    msk = np.array(mask.convert("L"))     # shape: (H, W)

    # Convert to (C, H, W)
    img = img.transpose(2, 0, 1)           # (3, H, W)
    msk = np.expand_dims(msk, axis=0)     # (1, H, W)

    # Pad
    img_padded = pad_img_to_modulo(img, 8)
    msk_padded = pad_img_to_modulo(msk, 8)

    # âœ… Convert to torch.Tensor vÃ  normalize
    img_tensor = torch.from_numpy(img_padded / 255.0).float().unsqueeze(0).to(device)  # (1, 3, H, W)
    msk_tensor = torch.from_numpy(msk_padded / 255.0).float().unsqueeze(0).to(device)  # (1, 1, H, W)

    batch = {
        'image': img_tensor,
        'mask': msk_tensor,
    }

    with torch.no_grad():
        result = model(batch)['inpainted'][0]  # (3, H, W)
        result = result.permute(1, 2, 0).cpu().numpy()  # â†’ (H, W, 3)
        result = np.clip(result * 255, 0, 255).astype('uint8')

    return Image.fromarray(result)